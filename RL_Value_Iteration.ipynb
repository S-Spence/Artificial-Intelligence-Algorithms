{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning with Value Iteration\n",
    "\n",
    "These are the same maps from the A\\* search notebook, but the \"physics\" of the world have changed. In the previous notebook, the world was deterministic. When the agent moved \"south\", it went \"south\". When it moved \"east\", it went \"east\". Now, the agent only succeeds in going where it wants to go *sometimes*. There is a probability distribution over the possible states so that when the agent moves \"south\", there is a small probability that it will go \"east\", \"north\", or \"west\" instead and have to move from there.\n",
    "\n",
    "There are a variety of ways to handle this problem. For example, if using A\\* search, if the agent finds itself off the solution, you can simply calculate a new solution from where the agent ended up. Although this sounds like a really bad idea, it has actually been shown to work really well in video games that use formal planning algorithms. When these algorithms were first designed, this was unthinkable. Thank you, Moore's Law!\n",
    "\n",
    "Another approach is to use Reinforcement Learning which covers problems where there is some kind of general uncertainty in the actions. We're going to model that uncertainty a bit unrealistically here to demonstrate how the algorithm works.\n",
    "\n",
    "As far as RL is concerned, there are a variety of options: model-based and model-free, Value Iteration, Q-Learning and SARSA. Ths notebook demonstrates Value Iteration.\n",
    "\n",
    "(above description by S. Butcher, 2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The World Representation\n",
    "\n",
    "The symbols that form the grid have a special meaning as they specify the type of the terrain and the cost to enter a grid cell with that type of terrain:\n",
    "\n",
    "```\n",
    "token   terrain    cost \n",
    ".       plains     1\n",
    "*       forest     3\n",
    "^       hills      5\n",
    "~       swamp      7\n",
    "x       mountains  impassible\n",
    "```\n",
    "\n",
    "When you go from a plains node to a forest node it costs 3. When you go from a forest node to a plains node, it costs 1. You can think of the grid as a big graph. Each grid cell (terrain symbol) is a node and there are edges to the north, south, east and west (except at the edges).\n",
    "\n",
    "There are quite a few differences between A\\* Search and Reinforcement Learning but one of the most salient is that A\\* Search returns a plan of N steps that gets us from A to Z, for example, A->C->E->G.... Reinforcement Learning, on the other hand, returns  a *policy* that tells us the best thing to do in **every state.**\n",
    "\n",
    "For example, the policy might say that the best thing to do in A is go to C. However, we might find ourselves in D instead. But the policy covers this possibility, it might say, D->E. Trying this action might land us in C and the policy will say, C->E, etc. At least with offline learning, everything will be learned in advance (in online learning, you can only learn by doing and so you may act according to a known but suboptimal policy).\n",
    "\n",
    "Nevertheless, if you were asked for a \"best case\" plan from (0, 0) to (n-1, n-1), you could (and will) be able to read it off the policy because there is a best action for every state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the same costs as before. Note that we've negated them this time because RL requires negative costs and positive rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': -1, '*': -3, '^': -5, '~': -7}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "costs = { '.': -1, '*': -3, '^': -5, '~': -7}\n",
    "costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and a list of offsets for `cardinal_moves`. You'll need to work this into your **actions**, A, parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cardinal_moves = [(0,-1), (1,0), (0,1), (-1,0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Value Iteration, we require knowledge of the *transition* function, as a probability distribution.\n",
    "\n",
    "The transition function, T, for this problem is 0.70 for the desired direction, and 0.10 each for the other possible directions. That is, if the agent selects \"north\" then 70% of the time, it will go \"north\" but 10% of the time it will go \"east\", 10% of the time it will go \"west\", and 10% of the time it will go \"south\". If agent is at the edge of the map, it simply bounces back to the current state.\n",
    "\n",
    "(above descriptions by S. butcher, 2022)\n",
    "\n",
    "The following cells implement the value iteration algorithm and return a policy describing what the agent should do in **every** board state. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pprint import pprint\n",
    "from typing import Dict, Tuple, List\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function provided by S. butcher, 2022\n",
    "def read_world(filename):\n",
    "    result = []\n",
    "    with open(filename) as f:\n",
    "        for line in f.readlines():\n",
    "            if len(line) > 0:\n",
    "                result.append(list(line.strip()))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"get_rewards\"></a>\n",
    "## get_rewards\n",
    "\n",
    "*The get_rewards function determines the reward or cost of taking an action at a specific state. This function returns the reward if the action results in the goal state or the cost of moving to that terrain for a non-terminal action.* **Used by**: [value_iteration](#value_iteration)\n",
    "\n",
    "* **world** List[List[str]]: the game world  \n",
    "* **current_position** Tuple[int]: the row and column of the current position\n",
    "* **action** Tuple[int]: the current action provided as an offset to the current position\n",
    "* **goal** Tuple[int]: the goal state\n",
    "* **costs** Dict[str, int]: the costs for different terrains in the game world\n",
    "* **reward** int: the reward for reaching the goal state\n",
    "\n",
    "**returns** int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(world: List[List[str]], current_position: Tuple[int], action: Tuple[int], goal: Tuple[int], costs: dict[str, int], reward: int) -> int:\n",
    "    row, col = current_position[0], current_position[1]\n",
    "    position = (row + action[0], col + action[1])\n",
    "    if position == goal:\n",
    "        return reward\n",
    "    world_state = world[position[0]][position[1]]\n",
    "    return costs[world_state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assertions/ unit tests\n",
    "test_world = [\n",
    "    [\"*\", \"*\", \"*\"],\n",
    "    [\"~\", \"~\", \".\"],\n",
    "    [\".\", \".\", \".\"]\n",
    "]\n",
    "\n",
    "assert get_reward(test_world, (0, 0), (0, 1), (2, 2), costs, 10) == -3\n",
    "# goal state\n",
    "assert get_reward(test_world, (2, 1), (0, 1), (2, 2), costs, 10) == 10\n",
    "assert get_reward(test_world, (0, 0), (1, 0), (2, 2), costs, 10) == -7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"get_valid_actions\"></a>\n",
    "## get_valid_actions\n",
    "\n",
    "*The get_valid_actions function takes a list of possible actions, the current position (as row and col parameters), and the world. The function applies each action to the current state and returns a list of actions that do not result in an out-of-bounds or invalid position.* **Used by**: [value_iteration](#value_iteration)\n",
    "\n",
    "* **actions** List[Tuple[int]]: the possible actions as a list of tuples containing the position offsets for the action (ex. (-1, 0) is the up action)\n",
    "* **current_position** Tuple[int]: the row and column of the current state\n",
    "* **world** List[List[str]]: the game world  \n",
    "\n",
    "**returns** List[Tuple[int]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_actions(actions: List[Tuple[int]], current_position: Tuple[int], world: List[List[int]]) -> List[Tuple[int]]:\n",
    "    row, col = current_position[0], current_position[1]\n",
    "    valid_actions = []\n",
    "    for action in actions:\n",
    "        position = (row + action[0], col + action[1])\n",
    "        if position[0] < len(world) and position[0] >= 0 and position[1] < len(world[0]) and position[1] >= 0 and world[position[0]][position[1]] != 'x':\n",
    "            valid_actions.append(action)\n",
    "    return valid_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assertions/ unit tests\n",
    "test_world = [\n",
    "    [\"*\", \"*\", \"*\"],\n",
    "    [\"~\", \"~\", \".\"],\n",
    "    [\".\", \".\", \".\"]\n",
    "]\n",
    "\n",
    "assert get_valid_actions(cardinal_moves, (0,0), test_world) == [(1,0), (0, 1)]\n",
    "assert get_valid_actions(cardinal_moves, (1,1), test_world) == cardinal_moves\n",
    "assert get_valid_actions(cardinal_moves, (2,0), test_world) == [(0,1), (-1, 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"get_max_delta\"></a>\n",
    "## get_max_delta\n",
    "\n",
    "*The get_max_delta function takes the values produced by the current iteration of the value iteration algorithm and compares them (state by state) to the values produced by the previous iteration. The function returns the maximum change (of any board state) between the previous and current iterations.* **Used by**: [value_iteration](#value_iteration)\n",
    "\n",
    "* **v** List[List[float]]: the values produced by the current iteration of the value iteration algorithm\n",
    "* **v_previous** List[List[float]]: the values produced by the previous iteration of the value iteration algorithm\n",
    "\n",
    "**returns** float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_delta(v: List[List[float]], v_previous: List[List[float]]) -> float:\n",
    "    differences = []\n",
    "    for sublist1, sublist2 in zip(v, v_previous):\n",
    "        for num1, num2 in zip(sublist1, sublist2):\n",
    "            differences.append(abs(num1 - num2))\n",
    "    return max(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assertions/ unit tests\n",
    "prev_test_values_1 = [[2, 3],[4, 5]]\n",
    "test_values_1 = [[1, -2], [1, 1]]\n",
    "\n",
    "assert get_max_delta(test_values_1, prev_test_values_1) == 5\n",
    "\n",
    "prev_test_values_2 = [[2, 3],[4, 5]]\n",
    "test_values_2 = [[2, 3], [4, 5]]\n",
    "\n",
    "assert get_max_delta(test_values_2, prev_test_values_2) == 0\n",
    "\n",
    "prev_test_values_3 = [[2, 3],[4, 5]]\n",
    "test_values_3 = [[3, 3], [4, 5]]\n",
    "\n",
    "assert get_max_delta(test_values_3, prev_test_values_3) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"apply_transition_model\"></a>\n",
    "## apply_transition_model\n",
    "\n",
    "*The apply_transition_model function calculates the value of an action in the current state. This transition model is for the stochastic value iteration algorithm, and it applies the prob_desired and prob_other parameters in the value calculation to account for the agent making an unexpected move in the current state. The algorithm calculates the value for the expected action as (prob_desired * the value of the resulting state in v_previous). The algorithm also calculates the summation of (prob_other * the value of the resulting state in v_previous) for all actions other than the current action. Not all actions are valid moves. Therefore, this function makes a static calculation of (prob_other * -10) for invalid moves from the current state, such as going out-of-bounds or moving to an invalid board state. The function adds these static calculations to the summation of unexpected actions. The function returns gamma (the discount factor) multiplied by (expected + unexpected).* **Used by**: [value_iteration](#value_iteration)\n",
    "\n",
    "* **gamma** float: the discount factor\n",
    "* **current_position** Tuple[int]: the row and column of the current state\n",
    "* **v_previous** List[List[float]]: the values produced by the previous iteration of the value iteration algorithm\n",
    "* **prob_desired** float: the probability of the agent taking the current action\n",
    "* **prob_other** float: the probability of the agent taking an action that is not the current action\n",
    "* **action** Tuple[int]: the current action\n",
    "* **valid_actions** List[Tuple[int]]: a list of valid actions from the current state\n",
    "* **actions** List[Tuple[int]]: a list of all possible \n",
    "\n",
    "**returns** float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transition_model(gamma: float, current_position: Tuple[int], v_previous: List[List[float]], prob_desired: float, prob_other: float, action: Tuple[int], valid_actions: List[Tuple[int]], actions: List[Tuple[int]]) -> float:\n",
    "    row, col = current_position[0], current_position[1]\n",
    "    position = (row + action[0], current_position[1] + col)\n",
    "    expected = prob_desired * v_previous[row+action[0]][col+action[1]]\n",
    "    unexpected = sum([prob_other * v_previous[row+curr_action[0]][col+curr_action[1]] for curr_action in valid_actions if curr_action != action])\n",
    "    # calculate the probability of bouncing off an invalid action as a static cost greater than the worst valid action\n",
    "    for i in range(len(actions) - len(valid_actions)):\n",
    "        unexpected += prob_other * -10\n",
    "    return gamma * (expected + unexpected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assertions/ unit tests\n",
    "v_previous_1 = [[5.00, 0.00, 0.00, 0.00, 0.00, 0.00, 5.00]]\n",
    "\n",
    "assert apply_transition_model(0.9, (0, 1), v_previous_1, 0.9, 0.1, (0, -1), [(0, -1), (0, 1)], [(0, -1), (0, 1)]) == 4.05\n",
    "\n",
    "v_previous_2 = [[5.00, 4.05, 0.00, 0.00, 0.00, 4.05, 5.00]]\n",
    "\n",
    "assert apply_transition_model(0.9, (0, 2), v_previous_2, 0.9, 0.1, (0, -1), [(0, -1), (0, 1)], [(0, -1), (0, 1)]) == 3.2805\n",
    "\n",
    "v_previous_3 = [[5.00, 4.05, 3.2805, 0.00, 3.2805, 4.05, 5.00]]\n",
    "\n",
    "assert round(apply_transition_model(0.9, (0, 3), v_previous_3, 0.9, 0.1, (0, -1), [(0, -1), (0, 1)], [(0, -1), (0, 1)]), 2) == 2.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"value_iteration\"></a>\n",
    "## value_iteration\n",
    "\n",
    "*The value_iteration function applies the stochastic value iteration algorithm to return a policy reflecting the best move for every board state, given a goal state. The value iteration algorithm finds the optimal path to a goal when run until convergence. The algorithm terminates when the maximum change between any cell's previous and current values is less than the epsilon parameter.*\n",
    "\n",
    "* **world** List[List[str]]: the current world\n",
    "* **costs** Dict[str, int]: the costs of non-terminal actions\n",
    "* **goal** Tuple[int]: the coordinates of the goal state\n",
    "* **rewards** int: the reward or payout for reaching the goal state\n",
    "* **actions** List[Tuple[int]]: a list of all possible actions\n",
    "* **gamma** float: the discount factor used in value calculations\n",
    "* **epsilon** float: the rate of change that determines convergence. The algorithm returns a policy when the maximum change between the previous values and the current values is less than epsilon. \n",
    "\n",
    "**returns** float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(world, costs, goal, rewards, actions, gamma, epsilon):\n",
    "    v_previous = [[0 for i in world[0]] for row in world]\n",
    "    iterations = 0\n",
    "    while True:\n",
    "        policy = {}\n",
    "        v = [[0 for i in world[0]] for row in world]\n",
    "        for row in range(len(world)):\n",
    "            for col in range(len(world[0])):\n",
    "                q = {}\n",
    "                if world[row][col] == \"x\":\n",
    "                    policy[(row, col)] = None\n",
    "                    continue\n",
    "                if (row, col) == goal:\n",
    "                    policy[(row, col)] = (0, 0)\n",
    "                    continue\n",
    "                    \n",
    "                valid_actions = get_valid_actions(actions, (row, col), world)\n",
    "                for action in valid_actions:\n",
    "                    action_payout = get_reward(world, (row, col), action, goal, costs, reward)\n",
    "                    q[action] = action_payout + apply_transition_model(gamma, (row, col), v_previous, 0.70, 0.10, action, valid_actions, actions)\n",
    "                policy[(row, col)] = max(q, key=lambda k: q[k])\n",
    "                v[row][col] = q[policy[(row, col)]]\n",
    "        max_delta = get_max_delta(v, v_previous)\n",
    "        if max_delta < epsilon:\n",
    "            print(f\"Iterations: {iterations}, Maximum Change: {max_delta}\") \n",
    "            return policy\n",
    "        iterations += 1\n",
    "        v_previous = deepcopy(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pretty_print_policy\"></a>\n",
    "## pretty_print_policy\n",
    "\n",
    "*The pretty_print_policy function takes the policy returned by the value iteration algorithm and prints the action the agent should take for every board state.*\n",
    "\n",
    "* **cols** int: the number of columns in the world\n",
    "* **rows** int: the number of rows in the world\n",
    "* **policy** Dict[Tuple[int], Tuple[int]: a policy that maps every board state to the best action for that state\n",
    "* **goal** Tuple[int]: the goal state\n",
    "\n",
    "**returns** None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_policy(cols: int, rows: int, policy: Dict[Tuple[int], Tuple[int]], goal: Tuple[int]) -> None:\n",
    "    symbols = {(0, 1): 'â©', (0, -1): 'âª', (-1, 0): 'â«', (1, 0): 'â¬', (0, 0): 'ğŸ'}\n",
    "    output = [[0 for col in range(cols)] for row in range(rows)]\n",
    "    for position, action in policy.items():\n",
    "        row, col = position[0], position[1]\n",
    "        output[row][col] = 'âŒ' if action == None else symbols[action]\n",
    "    # print the map by unpacking the list at each row\n",
    "    for row in output:\n",
    "        print(*row)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "\n",
    "### Small World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_world = read_world( \"Datasets/small.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 23, Maximum Change: 0.007250134098737426\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "goal = (len(small_world)-1, len(small_world[0])-1)\n",
    "gamma = 0.9\n",
    "reward = 10\n",
    "small_policy = value_iteration(small_world, costs, goal, reward, cardinal_moves, gamma, 0.01)\n",
    "assert len(small_policy) == len(small_world) * len(small_world[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬ â© â© â© â© â¬\n",
      "â¬ âª â« â« â© â¬\n",
      "â¬ âª â¬ â© â© â¬\n",
      "â¬ âª â¬ âŒ â© â¬\n",
      "â¬ â¬ â¬ â¬ â¬ â¬\n",
      "â© â© â© â© â¬ â¬\n",
      "â© â© â© â© â© ğŸ\n"
     ]
    }
   ],
   "source": [
    "cols = len(small_world[0])\n",
    "rows = len(small_world)\n",
    "pretty_print_policy(cols, rows, small_policy, goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_world = read_world(\"Datasets/large.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 100, Maximum Change: 0.00822413675506084\n"
     ]
    }
   ],
   "source": [
    "goal = (len(large_world)-1, len(large_world[0])-1) # Lower Right Corner FILL ME IN\n",
    "gamma = 0.9\n",
    "reward = 30000\n",
    "large_policy = value_iteration(large_world, costs, goal, reward, cardinal_moves, gamma, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬ â¬ â¬ â¬ âª âª â¬ â© â¬ â¬ â¬ â¬ â¬ â¬ â¬ â¬ â¬ âª âª âª â© â© â© â© â© â¬ â¬\n",
      "â¬ â¬ â¬ âª âª â© âª â© â© â© â© â© â© â¬ â¬ â¬ â¬ âª âŒ âŒ âŒ âŒ âŒ âŒ âŒ â¬ â¬\n",
      "â¬ â¬ â¬ âª âŒ âŒ â© â© â¬ â© â© â© â© â© â¬ â¬ â¬ âŒ âŒ âŒ â¬ â¬ âª âŒ âŒ â¬ â¬\n",
      "â¬ â¬ âª âª âª âŒ âŒ âŒ â¬ â¬ â© â© â© â© â© â¬ â¬ â¬ â¬ â¬ â¬ â¬ âª âŒ âŒ â¬ â¬\n",
      "â¬ âª âª âª âª âŒ âŒ â¬ â¬ â¬ â¬ â© â© â© â© â© â© â¬ â¬ â¬ â¬ â¬ âŒ âŒ âŒ â¬ â¬\n",
      "â¬ âª â« âª âŒ âŒ â¬ â¬ â¬ â¬ âª â© â© â© â© â© â© â© â© â¬ â¬ â¬ â¬ âŒ â¬ â¬ âª\n",
      "â¬ â¬ â¬ âŒ âŒ â© â© â¬ â¬ â¬ âª âª âŒ âŒ âŒ â© â© â© â© â© â© â¬ â¬ â¬ â¬ â¬ âª\n",
      "â© â¬ â¬ â© â© â¬ â¬ â¬ â¬ âª âª âª âª âª âŒ âŒ âŒ â© â© â© â© â© â© â¬ â¬ â¬ âª\n",
      "â© â© â© â© â© â© â¬ â¬ âª âª âª âª âª âª âŒ âŒ â© â© â© â© â© â© â© â¬ â¬ â¬ âª\n",
      "â© â© â« â© â© â© â© â¬ âª âª âª âŒ âŒ âŒ âŒ â© â© â¬ â¬ â© â© â© â© â© â¬ â¬ âª\n",
      "â© â« â© â© â© â¬ â¬ â¬ â¬ âª âŒ âŒ âŒ â¬ â© â© â© â© â¬ â¬ âŒ âŒ âŒ â© â¬ â¬ âª\n",
      "â« â© â© â© â© â¬ â¬ â¬ â¬ âŒ âŒ â¬ â© â© â© â© â© â© â¬ â¬ â¬ âŒ âŒ â¬ â¬ â¬ âª\n",
      "â¬ â© â© â© â© â© â¬ â¬ â¬ âŒ âŒ â¬ â¬ â© â© â© â© â© â© â¬ â¬ âŒ â© â¬ â¬ â¬ âª\n",
      "â¬ â© â© â© â© â© â¬ â¬ â¬ â¬ â¬ â¬ âª â© â© â© â© â© â© â© â© â© â© â© â¬ â¬ âª\n",
      "â© â© â© â« âŒ â© â© â© â¬ â¬ â¬ â¬ âª â« â© â© â© â© â© â© â« â« âŒ â© â¬ â¬ âª\n",
      "â© â« â« âŒ âŒ âŒ â© â© â¬ â¬ â¬ â¬ âŒ âŒ âŒ â« â© â« â« â« â« âŒ âŒ â¬ â¬ â¬ âª\n",
      "â© â« âŒ âŒ â© â© â© â© â© â© â¬ â¬ â¬ â¬ âŒ âŒ âŒ â« â« âŒ âŒ âŒ â¬ â¬ â¬ â¬ âª\n",
      "â© â¬ â¬ âŒ âŒ â© â© â© â© â© â© â© â¬ â¬ â¬ â¬ âŒ âŒ âŒ âŒ â¬ â¬ â¬ â¬ â¬ â¬ âª\n",
      "â© â¬ â¬ âŒ âŒ âŒ â© â© â© â© â© â© â© â© â© â¬ â¬ â¬ â¬ â¬ â¬ â¬ â¬ â¬ â¬ â¬ âª\n",
      "â© â¬ â¬ â¬ âŒ âŒ âŒ â© â© â© â© â© â© â© â© â© â© â¬ â¬ â© â© â© â¬ â¬ â¬ â¬ âª\n",
      "â© â¬ â¬ â¬ â¬ â¬ âŒ âŒ â« â© â© â« â« â« âŒ â© â© â© â© â© â© â© â¬ â¬ â¬ â¬ â¬\n",
      "â© â© â© â¬ â¬ â¬ â¬ âŒ âŒ âŒ â« â« âŒ âŒ â© â© â© â© â© â© â© â© â© â¬ â¬ â¬ â¬\n",
      "â© â© â© â© â© â© â¬ â¬ â¬ âŒ âŒ âŒ âŒ â© â© â© â© â© â© â© â© â© â© â¬ â¬ â¬ â¬\n",
      "â« â© â© â© â© â© â© â© â© â© â© â© â© â© â© â« â« âŒ âŒ â© â© â© â© â© â¬ â¬ â¬\n",
      "â« âŒ â© â© â© â© â© â© â© â© â« âŒ âŒ âŒ â« â« âŒ âŒ â¬ âŒ âŒ â© â© â© â© â¬ â¬\n",
      "â« âŒ âŒ âŒ â© â© â© â© â« â« â« â¬ âŒ âŒ âŒ âŒ â© â© â¬ â¬ âŒ âŒ âŒ â© â© â¬ â¬\n",
      "â© â© â© â© â© â« â« â« â« â« â© â© â© â© â© â© â© â© â© â© â© â© â© â© â© â© ğŸ\n"
     ]
    }
   ],
   "source": [
    "cols = len(large_world[0])\n",
    "rows = len(large_world)\n",
    "\n",
    "pretty_print_policy( cols, rows, large_policy, goal)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (en605645)",
   "language": "python",
   "name": "en605645"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "171px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
